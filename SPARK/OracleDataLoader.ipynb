{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e8dc28-7c9b-4d23-a2e4-2a000e58c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.0)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3398cf0a-da0e-4f30-a664-32716e2b656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91b7ac5-884a-444d-b3e5-cd5c8207f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleDataLoader:\n",
    "    def __init__(self,\n",
    "                 spark_session: SparkSession,\n",
    "                 jdbc_url: str,\n",
    "                 username: str,\n",
    "                 password: str):\n",
    "        \"\"\"\n",
    "        Инициализация загрузчика данных\n",
    "\n",
    "        :param spark_session: Spark сессия\n",
    "        :param jdbc_url: URL подключения к базе данных\n",
    "        :param username: Имя пользователя БД\n",
    "        :param password: Пароль пользователя БД\n",
    "        \"\"\"\n",
    "        self.spark = spark_session\n",
    "        self.jdbc_url = jdbc_url\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "        # Параметры конфигурации\n",
    "        self.config = {\n",
    "            'num_threads': 8,\n",
    "            'chunk_count': 512,\n",
    "            'fetch_size': 100000,\n",
    "            'target_num_files': 400\n",
    "        }\n",
    "\n",
    "    def _generate_extent_query(self, schema: str, table: str) -> str:\n",
    "        \"\"\"\n",
    "        Генерация запроса для получения информации о экстентах таблицы\n",
    "\n",
    "        :param schema: Схема БД\n",
    "        :param table: Имя таблицы\n",
    "        :return: SQL запрос\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        (SELECT\n",
    "            data_object_id,\n",
    "            file_id,\n",
    "            relative_fno,\n",
    "            file_batch,\n",
    "            subobject_name,\n",
    "            MIN(start_block_id) start_block_id,\n",
    "            MAX(end_block_id) end_block_id,\n",
    "            SUM(blocks) blocks\n",
    "        FROM\n",
    "            (SELECT\n",
    "                o.data_object_id,\n",
    "                o.subobject_name,\n",
    "                e.file_id,\n",
    "                e.relative_fno,\n",
    "                e.block_id start_block_id,\n",
    "                e.block_id + e.blocks - 1 end_block_id,\n",
    "                e.blocks,\n",
    "                CEIL(SUM(e.blocks) OVER (PARTITION BY o.data_object_id, e.file_id ORDER BY e.block_id ASC) /\n",
    "                    (SUM(e.blocks) OVER (PARTITION BY o.data_object_id, e.file_id) / {self.config['chunk_count']})) file_batch\n",
    "            FROM\n",
    "                dba_extents e,\n",
    "                dba_objects o,\n",
    "                dba_tab_subpartitions tsp\n",
    "            WHERE\n",
    "                o.owner = '{schema}'\n",
    "                AND o.object_name = '{table}'\n",
    "                AND e.owner = '{schema}'\n",
    "                AND e.segment_name = '{table}'\n",
    "                AND o.owner = e.owner\n",
    "                AND o.object_name = e.segment_name\n",
    "                AND (o.subobject_name = e.partition_name\n",
    "                    OR (o.subobject_name IS NULL AND e.partition_name IS NULL))\n",
    "                AND o.owner = tsp.table_owner(+)\n",
    "                AND o.object_name = tsp.table_name(+)\n",
    "                AND o.subobject_name = tsp.subpartition_name(+))\n",
    "        GROUP BY\n",
    "            data_object_id,\n",
    "            file_id,\n",
    "            relative_fno,\n",
    "            file_batch,\n",
    "            subobject_name\n",
    "        ORDER BY \n",
    "            data_object_id,\n",
    "            file_id,\n",
    "            relative_fno,\n",
    "            file_batch,\n",
    "            subobject_name)\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_rowid_query_udf(self):\n",
    "        \"\"\"\n",
    "        Создание UDF для генерации запросов по диапазону rowid\n",
    "\n",
    "        :return: UDF функция\n",
    "        \"\"\"\n",
    "\n",
    "        @udf(returnType=StringType())\n",
    "        def generate_rowid_query(data_object_id, relative_fno, start_block_id, end_block_id, columns):\n",
    "            columns_str = \", \".join(columns)\n",
    "            return f\"\"\"\n",
    "            SELECT /*+ NO_INDEX(t) */ {columns_str}\n",
    "            FROM table_name\n",
    "            WHERE (rowid >= dbms_rowid.rowid_create(1, {data_object_id}, {relative_fno}, {start_block_id}, 0)\n",
    "                   AND rowid <= dbms_rowid.rowid_create(1, {data_object_id}, {relative_fno}, {end_block_id}, 32767))\n",
    "            \"\"\"\n",
    "\n",
    "        return generate_rowid_query\n",
    "\n",
    "    def load_data(self,\n",
    "                  source_schema: str,\n",
    "                  source_table: str,\n",
    "                  target_path: str,\n",
    "                  target_table: str,\n",
    "                  partition: str = None):\n",
    "        \"\"\"\n",
    "        Основной метод загрузки данных\n",
    "\n",
    "        :param source_schema: Схема источника\n",
    "        :param source_table: Таблица источника\n",
    "        :param target_path: Путь для сохранения\n",
    "        :param target_table: Целевая таблица\n",
    "        :param partition: Партиция (опционально)\n",
    "        \"\"\"\n",
    "        # Получение информации об экстентах\n",
    "        extent_df = (self.spark.read\n",
    "                     .format(\"jdbc\")\n",
    "                     .option(\"url\", self.jdbc_url)\n",
    "                     .option(\"user\", self.username)\n",
    "                     .option(\"password\", self.password)\n",
    "                     .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n",
    "                     .option(\"dbtable\", self._generate_extent_query(source_schema, source_table))\n",
    "                     .load()\n",
    "                     )\n",
    "\n",
    "        # Фильтрация по партиции если указана\n",
    "        if partition:\n",
    "            extent_df = extent_df.filter(col(\"subobject_name\") == partition)\n",
    "\n",
    "        # Получение списка колонок\n",
    "        columns_df = (self.spark.read\n",
    "                      .format(\"jdbc\")\n",
    "                      .option(\"url\", self.jdbc_url)\n",
    "                      .option(\"user\", self.username)\n",
    "                      .option(\"password\", self.password)\n",
    "                      .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n",
    "                      .option(\"dbtable\", f\"(SELECT * FROM {source_schema}.{source_table} WHERE 1=0)\")\n",
    "                      .load()\n",
    "                      )\n",
    "        columns = columns_df.columns\n",
    "\n",
    "        # Создание UDF для генерации запросов\n",
    "        rowid_query_udf = self._create_rowid_query_udf()\n",
    "\n",
    "        # Генерация запросов для загрузки\n",
    "        queries_df = (extent_df\n",
    "                      .withColumn(\"query\",\n",
    "                                  rowid_query_udf(\n",
    "                                      col(\"data_object_id\").cast(IntegerType()),\n",
    "                                      col(\"relative_fno\").cast(IntegerType()),\n",
    "                                      col(\"start_block_id\").cast(IntegerType()),\n",
    "                                      col(\"end_block_id\").cast(IntegerType()),\n",
    "                                      lit(columns)\n",
    "                                  )\n",
    "                                  )\n",
    "                      .select(\"relative_fno\", \"file_batch\", \"query\")\n",
    "                      )\n",
    "\n",
    "        # Параллельная загрузка данных\n",
    "        def load_chunk(query_info):\n",
    "            relative_fno, file_batch, query = query_info\n",
    "            chunk_key = f\"{relative_fno}_{file_batch}\"\n",
    "\n",
    "            chunk_df = (self.spark.read\n",
    "                        .format(\"jdbc\")\n",
    "                        .option(\"url\", self.jdbc_url)\n",
    "                        .option(\"user\", self.username)\n",
    "                        .option(\"password\", self.password)\n",
    "                        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n",
    "                        .option(\"dbtable\", f\"({query})\")\n",
    "                        .option(\"fetchSize\", self.config['fetch_size'])\n",
    "                        .load()\n",
    "                        )\n",
    "\n",
    "            chunk_df.write.mode(\"overwrite\").orc(f\"{target_path}_tmp/part_{chunk_key}\")\n",
    "            return chunk_key\n",
    "\n",
    "        # Выполнение параллельной загрузки\n",
    "        with ThreadPoolExecutor(max_workers=self.config['num_threads']) as executor:\n",
    "            query_list = queries_df.collect()\n",
    "            futures = [executor.submit(load_chunk, (row.relative_fno, row.file_batch, row.query)) for row in query_list]\n",
    "\n",
    "            # Ожидание завершения загрузки\n",
    "            as_completed(futures)\n",
    "\n",
    "        # Объединение и сохранение результата\n",
    "        result_df = self.spark.read.orc(f\"{target_path}_tmp/*\")\n",
    "        (result_df\n",
    "         .coalesce(self.config['target_num_files'])\n",
    "         .write\n",
    "         .mode(\"overwrite\")\n",
    "         .option(\"path\", target_path)\n",
    "         .format(\"orc\")\n",
    "         .saveAsTable(target_table)\n",
    "         )\n",
    "\n",
    "        # Очистка временных файлов\n",
    "        import shutil\n",
    "        shutil.rmtree(f\"{target_path}_tmp\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a00bd-5429-40b6-8894-07feea0e8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Egor\").config(\"spark.master\", \"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0f482-53bf-4b45-9ddc-16318850bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:oracle:thin:@//oracle:1521/db\"\n",
    "username = \"sys\"\n",
    "password = \"admin\"\n",
    "\n",
    "# Создание загрузчика\n",
    "loader = OracleDataLoader(spark, jdbc_url, username, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae806ab-76f9-4761-bdae-ac6c16183016",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.load_data(\n",
    "    source_schema=\"SYS\",\n",
    "    source_table=\"LARGE_DATA_TABLE\",\n",
    "    target_path=\"/warehouse/tablespace/external/hive/customer360_stg.db/service_feature_orcl\",\n",
    "    target_table=\"customer360_stg.service_feature_orcl\",\n",
    "    partition=\"SERVICE_FEATURE_MAX\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7983c6a-ae2f-42cb-97d2-8a56044e0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
